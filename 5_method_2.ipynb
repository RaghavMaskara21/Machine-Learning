{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_method_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQSsYCuDmoG5M2p2mh7yHs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaghavMaskara21/Machine-Learning/blob/main/5_method_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cF9yax8QP72",
        "outputId": "a4a06d6f-677e-4adc-8c91-a404ca777cd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (0.6.3)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "40 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "enchant is already the newest version (1.6.0-11.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.2\n",
            "Collecting https://github.com/andreasvc/readability/tarball/master\n",
            "  Using cached https://github.com/andreasvc/readability/tarball/master\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pyspellchecker\n",
        "!apt update\n",
        "!apt install -qq enchant\n",
        "!pip install pyenchant\n",
        "!pip3 install https://github.com/andreasvc/readability/tarball/master\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "from __future__ import unicode_literals\n",
        "import spacy,en_core_web_sm\n",
        "from collections import Counter\n",
        "from spellchecker import SpellChecker    \n",
        "import enchant\n",
        "import readability\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "from enchant.checker import SpellChecker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "pd.set_option('mode.chained_assignment', None)"
      ],
      "metadata": {
        "id": "3qnetHgaQkE_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extractFeature(text,index):\n",
        "  c = Counter(([token.pos_ for token in nlp(text)]))\n",
        "  data['Nouns'][index] = c['NOUN'] if 'NOUN' in c else 0\n",
        "  data['Verbs'][index] = c['VERB'] if 'VERB' in c else 0\n",
        "  data['ProperNouns'][index] = c['PROPN'] if 'PROPN' in c else 0\n",
        "  data['Adjectives'][index] = c['ADJ'] if 'ADJ' in c else 0\n",
        "  data['Punctuation'][index] = c['PUNCT'] if 'PUNCT' in c else 0\n",
        "  \n",
        "  # get wrong words\n",
        "  wordlist=text.split()\n",
        "  chkr = SpellChecker(\"en_US\",text)\n",
        "  data['WrongWords'][index] = len(list(chkr))\n",
        "  d = enchant.Dict(\"en_US\")\n",
        "  data['DifficultWords'][index]= [d.check(w) for w in wordlist].count(False)\n",
        "  data['Length'][index] = len(text)\n",
        "  \n",
        "  data['StopWords'][index] = len(set(wordlist) & stop_words)\n",
        "  data['OneLetterWords'][index]=len([word for word in wordlist if len(word)==1])\n",
        "  data['TwoLetterWords'][index]=len([word for word in wordlist if len(word)==2])\n",
        "\n",
        "  #get readability index\n",
        "  results = readability.getmeasures(text, lang='en')\n",
        "  data['FleschReading'][index] = results['readability grades']['FleschReadingEase']\n",
        "  data['DaleChallIndex'][index] = results['readability grades']['DaleChallIndex']\n"
      ],
      "metadata": {
        "id": "FI_eXmBSQko-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"spam.csv\", encoding = \"latin-1\")\n",
        "data = data.dropna(how = \"any\", axis = 1)\n",
        "nlp = en_core_web_sm.load()\n",
        "index = 0\n",
        "data['Nouns'] = 0\n",
        "data['ProperNouns'] = 0\n",
        "data['Verbs'] = 0\n",
        "data['Adjectives'] = 0\n",
        "data['Punctuation'] = 0\n",
        "data['WrongWords'] = 0\n",
        "data['StopWords'] = 0\n",
        "data['Length'] = 0\n",
        "data['OneLetterWords'] = 0\n",
        "data['TwoLetterWords'] = 0\n",
        "data['DifficultWords'] = 0\n",
        "data['FleschReading'] = 0\n",
        "data['DaleChallIndex'] = 0\n",
        "for text in data['v2']:  \n",
        "  extractFeature(text,index)\n",
        "  #print(index)\n",
        "  index+=1\n",
        "data\n",
        "data.to_csv('spam_features.csv')\n"
      ],
      "metadata": {
        "id": "8HzAijFjQpcI"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}